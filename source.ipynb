{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/edvardohlstrom/Documents/UCLouvain/Big data in finance'"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pypfopt\n",
    "from pypfopt.expected_returns import mean_historical_return\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  number of column droped. The droped columns:  ['ME7 BM10', 'ME10 BM8', 'ME10 BM9', 'BIG HiBM']\n",
      "Read and cleaned data from path:  Data/100_Portfolios_10x10.CSV\n"
     ]
    }
   ],
   "source": [
    "# helper functions\n",
    "\n",
    "def read_clean_data(path):\n",
    "    df = pd.read_csv(path,low_memory=False)\n",
    "    df = df.dropna(axis=0) # removes rows with null values\n",
    "    if \"Daily\" in path: \n",
    "        df ['Date'] = pd.to_datetime(df ['Date'],format=\"%Y%m%d\")\n",
    "        df  = df [(df ['Date'] >= \"1970-01-01\") & (df ['Date'] <= \"2022-12-31\" )]\n",
    "        df ['Date'] = df ['Date'].dt.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        df['Date'] = pd.to_datetime(df['Date'],format=\"%Y%m\")\n",
    "        df = df[(df['Date'] >= \"1970-01\") & (df['Date'] <= \"2022-12\" )]\n",
    "\n",
    "    if df['Date'].duplicated().any() :\n",
    "        print(\"Contain dublicate dates\")\n",
    "\n",
    "    df  = df.drop(df.columns[0], axis=1) # remove date column\n",
    "    df = df.astype(float)\n",
    "\n",
    "    # remove all coulumns with -99.99\n",
    "    shape = int(df.shape[1])\n",
    "    columns = list(df.columns)\n",
    "    specific_value = -99.99\n",
    "    df = df.loc[:, ~df.isin([specific_value]).any()] \n",
    "    if shape != df.shape[1]:\n",
    "        print(shape-df.shape[1], \" number of column droped. The droped columns: \", [x for x in columns if x not in df.columns] )\n",
    "    \n",
    "    df  = df*(0.01) #scale to decimals from percentages\n",
    "\n",
    "    if df.isnull().values.any():\n",
    "        print(\"DataFrame contains null values\")\n",
    "    \n",
    "    non_numeric_df = df.select_dtypes(exclude=['number'])\n",
    "    non_numeric_values = [val for val in non_numeric_df.stack().tolist()]\n",
    "    if non_numeric_values == 0:\n",
    "        print(\"DataFrame contain non decimal values\")\n",
    "\n",
    "    print( \"DONE: Read and cleaned data from path: \", path)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_performance(data, weigth):\n",
    "    ret = np.sum((data*weigth).to_numpy()) *100\n",
    "    var = np.sqrt(np.var((data*weigth).to_numpy())) *100\n",
    "    sharp = ret/var\n",
    "    return ret, var, sharp\n",
    "\n",
    "def get_compareable_performance(data):\n",
    "    return (np.mean(data), np.var(data))\n",
    "\n",
    "\n",
    "def plot_perforamce(data_test, data_val, title=\"\"):\n",
    "    plt.figure()\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(8, 10))\n",
    "    axs[0].plot(np.array(data_test)[:,0])\n",
    "    axs[0].plot(np.array(data_val)[:,0])\n",
    "    axs[0].legend([\"Test\",\"Val\"])\n",
    "    axs[0].set_xlabel(\"Window\")\n",
    "    axs[0].set_ylabel(\"Annual return\")\n",
    "\n",
    "    axs[1].plot(np.array(data_test)[:,1])\n",
    "    axs[1].plot(np.array(data_val)[:,1])\n",
    "    axs[1].legend([\"Test\",\"Val\"])\n",
    "    axs[1].set_xlabel(\"Window\")\n",
    "    axs[1].set_ylabel(\"Volatility [std]\")\n",
    "\n",
    "    axs[2].plot(np.array(data_test)[:,2])\n",
    "    axs[2].plot(np.array(data_val)[:,2])\n",
    "    axs[2].legend([\"Test\",\"Val\"])\n",
    "    axs[2].set_xlabel(\"Window\")\n",
    "    axs[2].set_ylabel(\"Sharp\")\n",
    "\n",
    "    fig.suptitle(title, fontsize=14, fontweight=\"bold\")\n",
    "    plt.savefig(\"Pictures/\" + title)\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read and cleaned data from path:  Data/10_Industry_Portfolios.csv\n",
      "Read and cleaned data from path:  Data/25_Portfolios_5x5.CSV\n",
      "4  number of column droped. The droped columns:  ['ME7 BM10', 'ME10 BM8', 'ME10 BM9', 'BIG HiBM']\n",
      "Read and cleaned data from path:  Data/100_Portfolios_10x10.CSV\n",
      "Read and cleaned data from path:  Data/48_Industry_Portfolios.CSV\n",
      "Read and cleaned data from path:  Data/25_Portfolios_5x5_Daily.csv\n",
      "Read and cleaned data from path:  Data/10_Industry_Portfolios_Daily.csv\n",
      "4  number of column droped. The droped columns:  ['ME7 BM10', 'ME10 BM8', 'ME10 BM9', 'BIG HiBM']\n",
      "Read and cleaned data from path:  Data/100_Portfolios_10x10_Daily.csv\n",
      "Read and cleaned data from path:  Data/48_Industry_Portfolios_Daily.csv\n"
     ]
    }
   ],
   "source": [
    "## tests\n",
    "\n",
    "# Should be same shape part 1\n",
    "df_1 = read_clean_data(\"Data/10_Industry_Portfolios.csv\")\n",
    "df_2 = read_clean_data(\"Data/25_Portfolios_5x5.CSV\")\n",
    "df_3 = read_clean_data(\"Data/100_Portfolios_10x10.CSV\")\n",
    "df_4 = read_clean_data(\"Data/48_Industry_Portfolios.CSV\")\n",
    "\n",
    "if not (len(df_3) == len(df_2) == len(df_3) == len(df_4) ):\n",
    "    print(\"Datasets are different length\")\n",
    "\n",
    "# Should be same shape part 2\n",
    "df = read_clean_data(\"Data/25_Portfolios_5x5_Daily.csv\")\n",
    "df = read_clean_data(\"Data/10_Industry_Portfolios_Daily.csv\")\n",
    "df = read_clean_data(\"Data/100_Portfolios_10x10_Daily.csv\")\n",
    "df = read_clean_data(\"Data/48_Industry_Portfolios_Daily.csv\")\n",
    "\n",
    "if not (len(df_3) == len(df_2) == len(df_3) == len(df_4) ):\n",
    "    print(\"Datasets are different length\")\n",
    "\n",
    "del df_1, df_2, df_3, df_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10_Industry_Portfolios'"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"Data/10_Industry_Portfolios.csv\"\n",
    "p[5:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_analysis(path):\n",
    "    df = read_clean_data(path)\n",
    "    n = len(df)\n",
    "    year_idx = n//(2023-1970)\n",
    "    window_size = year_idx*10\n",
    "    index_increase = window_size//20\n",
    "    num_of_windows = 86 # (53 years - 10 years ) * 2  \n",
    "\n",
    "    # window_size = 2526\n",
    "    # index_increase = 126\n",
    "    # num_of_windows = 86 # (52 years - 10 years ) * 2\n",
    "\n",
    "    min_var_test = []\n",
    "    min_var_val = []\n",
    "\n",
    "    equal_weight_test = []\n",
    "    equal_weight_val = []\n",
    "\n",
    "    my_list = df.columns.to_list()\n",
    "    equal_weights = {k: 1/len(my_list) for k in my_list}\n",
    "\n",
    "    mean_var_test = []\n",
    "    mean_var_val = []\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(num_of_windows):\n",
    "        temp = df.iloc[(i*index_increase):(window_size+i*index_increase)]\n",
    "\n",
    "        mu = mean_historical_return(temp, returns_data=True, compounding=False)\n",
    "        S = risk_models.sample_cov(temp, returns_data=True)\n",
    "\n",
    "        ef_min = EfficientFrontier(mu, S)\n",
    "        ef_min.min_volatility()\n",
    "\n",
    "        ef_equal = EfficientFrontier(mu, S)\n",
    "        ef_equal.set_weights(equal_weights)\n",
    "\n",
    "        ef_mean = EfficientFrontier(mu, S)\n",
    "        mean_var = np.sqrt(np.mean(S.values))\n",
    "        ef_mean.efficient_risk(mean_var)\n",
    "\n",
    "        \n",
    "        equal_weigth_ret_test, equal_weigth_var_test, equal_weigth_sharp_test = get_performance(temp, ef_equal.weights)\n",
    "        equal_weight_test.append([equal_weigth_ret_test*0.1, equal_weigth_var_test*0.1, equal_weigth_sharp_test])\n",
    "\n",
    "        min_var_ret_test, min_var_var_test, min_var_sharp_test = get_performance(temp, ef_min.weights)\n",
    "        min_var_test.append([min_var_ret_test*0.1, min_var_var_test*0.1, min_var_sharp_test])\n",
    "            \n",
    "        mean_var_ret_test, mean_var_var_test, mean_var_sharp_test = get_performance(temp, ef_mean.weights)\n",
    "        mean_var_test.append([mean_var_ret_test*0.1, mean_var_var_test*0.1, mean_var_sharp_test])\n",
    "        \n",
    "        \n",
    "        temp2 = df.iloc[(window_size+(i)*index_increase):(window_size+(i+1)*index_increase)]\n",
    "        \n",
    "        equal_weigth_ret_val, equal_weigth_var_val, equal_weigth_sharp_val = get_performance(temp2, ef_equal.weights)\n",
    "        equal_weight_val.append([equal_weigth_ret_val*2, equal_weigth_var_val*2, equal_weigth_sharp_val])\n",
    "        \n",
    "        min_var_ret_val, min_var_var_val, min_var_sharp_val = get_performance(temp2, ef_min.weights)\n",
    "        min_var_val.append([min_var_ret_val*2, min_var_var_val*2, min_var_sharp_val])\n",
    "\n",
    "        mean_var_ret_val, mean_var_var_val, mean_var_sharp_val = get_performance(temp2, ef_mean.weights)\n",
    "        mean_var_val.append([mean_var_ret_val*2, mean_var_var_val*2, mean_var_sharp_val])\n",
    "    \n",
    "\n",
    "\n",
    "    plot_perforamce(min_var_test,min_var_val, \"Minimum variance portfolio \" + path[5:-4])\n",
    "    plot_perforamce(equal_weight_test ,equal_weight_val, \"Equal weight portfolio\"  + path[5:-4])\n",
    "    plot_perforamce(mean_var_test, mean_var_val, \"Mean variance portfolio\" + path[5:-4])\n",
    "\n",
    "\n",
    "    return [get_compareable_performance(min_var_test), get_compareable_performance(min_var_val),\\\n",
    "        get_compareable_performance(equal_weight_test), get_compareable_performance(equal_weight_val),\\\n",
    "        get_compareable_performance(mean_var_ret_test), get_compareable_performance(mean_var_ret_val)]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea data representation\n",
    "\n",
    "\n",
    "datasets         -  10_Industry_Portfolios.csv - Data/25_Portfolios_5x5.CSV - 100_Portfolios_10x10.CSV\n",
    "Portfolios\n",
    "\n",
    "mean_variance_test       mean, variance            mean, variance                mean, variance\n",
    "mean_variance_val        mean, variance            mean, variance                mean, variance\n",
    "\n",
    "min_variance_test        mean, variance            mean, variance                mean, variance\n",
    "min_variance_val         mean, variance            mean, variance                mean, variance\n",
    "\n",
    "equal_variance_test      mean, variance            mean, variance                mean, variance\n",
    "equal_variance_val       mean, variance            mean, variance                mean, variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = [(\"Data/10_Industry_Portfolios.csv\"), (\"Data/25_Portfolios_5x5.CSV\"),\\\n",
    "             (\"Data/100_Portfolios_10x10.CSV\"),(\"Data/48_Industry_Portfolios.CSV\"), \\\n",
    "             (\"Data/25_Portfolios_5x5_Daily.csv\"),(\"Data/10_Industry_Portfolios_Daily.csv\"), \\\n",
    "             (\"Data/100_Portfolios_10x10_Daily.csv\"),(\"Data/48_Industry_Portfolios_Daily.csv\")]\n",
    "\n",
    "variable = []\n",
    "\n",
    "for path in path_list:\n",
    "    variable.append(run_analysis(path))\n",
    "\n",
    "variable\n",
    "\n",
    "# run_analysis(\"Data/10_Industry_Portfolios.csv\")\n",
    "# run_analysis(\"Data/25_Portfolios_5x5.CSV\")\n",
    "# run_analysis(\"Data/100_Portfolios_10x10.CSV\")\n",
    "# run_analysis(\"Data/48_Industry_Portfolios.CSV\")\n",
    "# run_analysis(\"Data/25_Portfolios_5x5_Daily.csv\")\n",
    "# run_analysis(\"Data/10_Industry_Portfolios_Daily.csv\")\n",
    "# run_analysis(\"Data/100_Portfolios_10x10_Daily.csv\")\n",
    "# run_analysis(\"Data/48_Industry_Portfolios_Daily.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the in-sample and out-of-sample performance of three portfolio strategies: sample mean-variance portfolio, sample minimum-variance portfolio and equally weighted portfolio. For the in-sample performance, compute the three portfolios on all returns directly. For the out-of-sample performance, implement a rolling-window approach: use an estimation window of 10 years, compute the out-of-sample performance on the next six months, and then roll over the windows by six months until the end of the sample is achieved. Report the performance in terms of annualized mean, volatility and Sharpe ratio.\n",
    "\n",
    "Discuss your results and explain why you obtain such results. What is the impact of the return frequency (daily or monthly)? How stable are the portfolio weigths over time (e.g. by computing the turnover or plotting boxplots)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mu = mean_historical_return(df, returns_data=True, compounding=False, frequency=252)\n",
    "S = risk_models.sample_cov(df, returns_data=True, frequency=252)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected annual return: 12.3%\n",
      "Annual volatility: 13.9%\n",
      "Sharpe Ratio: 0.74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.12259656566556762, 0.13893216657765387, 0.7384651675191644)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max sharpe portofolio\n",
    "ef = EfficientFrontier(mu, S)\n",
    "\n",
    "ef.max_sharpe()\n",
    "ef.portfolio_performance(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected annual return: 13.0%\n",
      "Annual volatility: 16.1%\n",
      "Sharpe Ratio: 0.68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16110944541202776"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ef = EfficientFrontier(mu, S)\n",
    "mean_var = np.sqrt(np.mean(S.values))\n",
    "ef.efficient_risk(mean_var)\n",
    "ef.portfolio_performance(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected annual return: 11.7%\n",
      "Annual volatility: 13.5%\n",
      "Sharpe Ratio: 0.72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.11743136072167151, 0.1353849944154488, 0.7196614450689345)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample minimum-variance portfolio\n",
    "ef = EfficientFrontier(mu, S)\n",
    "ef.min_volatility() \n",
    "ef.portfolio_performance(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected annual return: 11.9%\n",
      "Annual volatility: 16.1%\n",
      "Sharpe Ratio: 0.61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.11863471613434065, 0.16110944541202776, 0.6122218091067737)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equally weighted portfolio\n",
    "ef = EfficientFrontier(mu, S)\n",
    "\n",
    "\n",
    "my_list = df.columns.to_list()\n",
    "my_dict = {k: 0.1 for k in my_list}\n",
    "\n",
    "ef.set_weights(my_dict)\n",
    "ef.portfolio_performance(verbose=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
